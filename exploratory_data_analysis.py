# -*- coding: utf-8 -*-
"""Exploratory Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BM11rrDFPQDTMQiOeJmNwbrVumIcNCYq
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 0.1.Import libraries"""

import warnings
warnings.simplefilter('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

import json
import csv
from progressbar import progressbar
import requests
import math

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/Shared drives/SI650/dataset/'

import datetime
# Convert back to datetime object
def convert_to_time_obj(timestamp):
  return datetime.datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.000+00:00')

"""## 1.Exploratory Data Analysis

## 1.1.Load dataset
"""

df_clean = pd.read_csv('sephora_review_skincare_db_id.csv')

df_clean.sample(10)

"""## 1.2.Cleaning data"""

df_clean = df_clean.drop_duplicates()

df_clean['pair_user_product'] = df_clean.user_id.astype(str) + "_" + df_clean.product_id
df_clean = df_clean.drop_duplicates(subset='pair_user_product')

df_clean.info()

# high ratings (rating 3-5) >> recommend
df_clean_high_rate = df_clean[df_clean['rating'] >= 4]
df_clean_high_rate.recommended.value_counts()

# low ratings (rating 1 and 2) >> not recommend
df_clean_low_rate = df_clean[~(df_clean['rating'] >= 4)]
df_clean_low_rate.recommended.value_counts()

# 255242 unique user_id
df_clean.user_id.value_counts()

df_clean.brand.value_counts()

"""# 1.2.Heatmap count of reviews by Month and Year"""

df_clean['first_datetime'] = df_clean['first_submission_date'].map(convert_to_time_obj)

"""The users have started to reviews since 2017"""

yearly_table = df_clean['first_datetime'].groupby([df_clean['first_datetime'].dt.year.rename('year'),
                                       df_clean['first_datetime'].dt.month.rename('month')]).agg({'count'})
yearly_table = yearly_table.reset_index().pivot('month', 'year', 'count')
f, ax = plt.subplots(figsize=(7, 5))
sns.heatmap(yearly_table, annot=False, fmt="f", linewidths=.5, ax=ax, cmap="Reds")
ax.axes.set_title("Heatmap of Count of reviews by Month and Year", fontsize=13, y=1.01);

"""They mostly reviewed products on Monday, Tueday, and Wednesday evening till early morning on the next day

# 1.3.Heatmap count of reviews by weekday and time
"""

weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
weekly_table = df_clean[df_clean['year'] == 2019]['first_datetime'].groupby([df_clean['first_datetime'].dt.weekday.rename('weekday'),
                                       df_clean['first_datetime'].dt.hour.rename('hour')]).agg({'count'})
weekly_table = weekly_table.reset_index().pivot('hour', 'weekday', 'count')

f, ax = plt.subplots(figsize=(7, 9))
ax = sns.heatmap(weekly_table, annot=True, fmt="d", linewidths=.5, ax=ax,
                 xticklabels=weekdays,  cmap="Reds")
ax.axes.set_title("Count of reviews by Day and Hour of Day in 2019",
                  fontsize=13, y=1.01)
ax.set(xlabel='Day of Week', ylabel='Hour')

"""# 1.4. The long-tail of product popularity"""

# Summary plot of words which have same frequency. 
# x - number of appearances, word_frequency
# y - how many words (frequency) appears in each particular number, number_such_words
def set_plot_frequency(tokens):
    
    # get dictionary (word: x)
    count_token = Counter(tokens)
    # get dictionary (x: y)
    counter_of_counts = Counter(count_token.values())
    # sorted by y 
    counter_of_counts = sorted(counter_of_counts.items(), key=lambda pair: pair[0])
    
    word_frequency = np.asarray(counter_of_counts)[:,0]
    number_such_words = np.asarray(counter_of_counts)[:,1]
    number_such_words = number_such_words.astype('int')
    #plot
    plt.figure(figsize=(15,5))

    plt.subplot(1, 2, 1)
    plt.plot(word_frequency, number_such_words)
    plt.xlabel('Product frequency')
    plt.ylabel('number of such products')
    plt.title('Product distribution')
    #plt.grid(True)
    
    plt.subplots_adjust(wspace=0.5)
    
    plt.subplot(1, 2, 2)
    plt.plot(np.log(word_frequency), np.log(number_such_words))
    plt.xlabel('Log of product frequency')
    plt.ylabel('Log of number of such products')
    plt.title('Power law for product frequencies')
    #plt.grid(True)

    return plt.show()

"""Create product_id list by the number of review_text"""

df_clean_drop = df_clean.dropna(subset=['review_text'])

count_token = Counter(prod_list)
sorted_count_token = sorted(count_token.items(), key = lambda pair: pair[1], reverse = True)
word_frequency = np.asarray(sorted_count_token)[:,0]
frequency = np.asarray(sorted_count_token)[:, 1]
frequency = frequency.astype('int')

plt.figure(figsize=(15,5))
# fig = plt.figure(figsize=(15,5))
plt.plot(word_frequency, frequency)
plt.xlabel('Product ID by rank', fontsize=12)
plt.ylabel('Popularity', fontsize=12)
plt.xticks(np.arange(0, len(word_frequency), 10), word_frequency, rotation=90, ha='left');
# plt.yticks(np.arange(0, len(frequency), 1000), frequency, va='bottom');
# 20% 
plt.plot(["P124716", "P124716"], [-1, max], '-.', c='orange', label="20th product_id")
# 80%
plt.plot(["P435378", "P435378"], [-1, max], '-.', c='blue', label="80th product_id")
plt.title('Product distribution', fontsize=20)
plt.legend()

# Short head
short_product_id = word_frequency[:57]
short_product_id

# Long tail
long_product_id = word_frequency[58:234]
long_product_id

# Distant tail
distant_product_id = word_frequency[235:]
distant_product_id

"""# 1.5. Check data in each group"""

df_short = df_clean_drop[df_clean_drop.product_id.isin(short_product_id)]
df_short.recommended.value_counts()

df_short.rating.value_counts()

df_long = df_clean_drop[df_clean_drop.product_id.isin(long_product_id)]
df_long.recommended.value_counts()

df_distant = df_clean_drop[df_clean_drop.product_id.isin(distant_product_id)]
df_distant.recommended.value_counts()

df_distant.rating.value_counts()

df_short.rating.value_counts()[5] / df_short.rating.value_counts().sum() * 100

df_long.rating.value_counts()[5] / df_long.rating.value_counts().sum() * 100

df_distant.rating.value_counts()[5] / df_distant.rating.value_counts().sum() * 100

"""# 1.6. Find most reviews skincare brand

"""

gr_brand_2 = pd.DataFrame(df_clean.groupby(['brand']).description.count())
# gr_brand['num'] = [sum(x) for x in gr_brand.name]
gr_brand_2 = gr_brand_2.sort_values('description', ascending=False)
gr_brand_2.head(5)

gr_brand_3 = pd.DataFrame(df_long.groupby(['brand']).description.count())
# gr_brand['num'] = [sum(x) for x in gr_brand.name]
gr_brand_3 = gr_brand_3.sort_values('description', ascending=False)
gr_brand_3.head(5)

gr_brand_4 = pd.DataFrame(df_short.groupby(['brand']).description.count())
# gr_brand['num'] = [sum(x) for x in gr_brand.name]
gr_brand_4 = gr_brand_4.sort_values('description', ascending=False)
gr_brand_4.head(5)

"""# 1.7.Find most reviews skincare brand and name"""

gr_brand = pd.DataFrame(df_clean.groupby(['brand', 'name']).description.count())
gr_brand = gr_brand.sort_values('description', ascending=False).reset_index()
gr_brand.head(10)

"""# 1.8. Filter out to get only recommended products"""

# most recommended data are missing >> so the data was lost in a half
target_ix = df_clean[df_clean.recommended==True]

prod_rec = target_ix.loc[:, ['brand', 'name', 'skin_tone', 'skin_concerns', 'recommended']]

gr_bran_pi = pd.DataFrame(prod_rec.groupby(['brand', 'name'], as_index= False).agg({
  'recommended' : "count"

}))
#  gr_brand['num'] = [sum(x) for x in gr_brand.name]
gr_bran_pi = gr_bran_pi.sort_values('recommended', ascending=False)
# gr_bran_pi.reset_index()
gr_bran_pi

gr_bran_pi['brand_name'] = gr_bran_pi['brand'] + " : " + gr_bran_pi['name']

plt.figure(figsize=(10,10), dpi=80)
sns.barplot(data=gr_bran_pi[0:20], x='recommended', y='brand_name',palette="Set2")
plt.title('Top 25 Product Recommended', fontsize=20)